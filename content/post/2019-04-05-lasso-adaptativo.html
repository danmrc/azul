---
title: 'LASSO Adaptativo '
author: Daniel Coutinho
date: '2019-04-05'
slug: lasso-adaptativo
categories:
  - Econometria
  - Machine Learning
tags:
  - LASSO
authors: ["danmrc"] 
draft: true
---



<p>Em um <a href="https://azul.netlify.com/2018/09/16/lasso/">post anterior</a>, eu falei do LASSO (Least Absolute Shrinkage and Select Operator). Como vamos explorar uma variação do LASSO hoje, eu vou repetir o problema que o LASSO resolvia:</p>
<p><span class="math display">\[\hat{\beta}_{LASSO} \in \arg \min_{\beta} \displaystyle \sum_{i=1}^n (y_i - x_i \beta)^2 + \lambda \sum_{k=0}^p |\beta_k|\]</span></p>
<p>(Onde <span class="math inline">\(|.|\)</span> é o valor absoluto do termo). E como eu já disse, o LASSO nos fornece uma maneira de selecionar quais variáveis entram no modelo ou não. Vamos fazer um pequeno teste com o LASSO: eu vou gerar 50 variáveis normais, independentes, e dessas dez - as dez primeiras - eu colocarei<span class="math inline">\(\beta=1\)</span>. As outra vão ser irrelevantes para o problema e vão ter <span class="math inline">\(\beta=0\)</span>. O tamanho da amostra vai ser igual a 100.</p>
<p>Como de praxe, nós podemos ter diversos objetivos ao estimar um modelo. Eu vou comparar 3 coisas: a quantidade de vezes que o LASSO coloca as variáveis relevantes, a quantidade de vezes que ele exclui as variáveis irrelevantes e quando ele obtém o modelo certo - o que exige colocar todo mundo que é relevante <strong>e</strong> excluir todos os irrelevantes. Vou fazer só 500 replicações e usar o Cross Validation para escolher o <span class="math inline">\(\lambda\)</span>:</p>
<pre class="r"><code>library(glmnet)

coeficientes &lt;- Matrix(0,ncol=500,nrow=51)

for(i in 1:500){
  x &lt;- matrix(rnorm(50*100),ncol = 50) #gerando x
  betas &lt;- c(rep(1,10),rep(0,40)) #
  y &lt;- x%*%betas + rnorm(100)
  modelo &lt;- cv.glmnet(x,y) #estimando usando LASSO e Cross Validation
  coeficientes[,i] &lt;- coef(modelo)
}

#Fim da simulação

analise &lt;- matrix(0,ncol=3,nrow=500)
colnames(analise)&lt;- c(&quot;Não zeros certos&quot;,&quot;Zeros certos&quot;, &quot;Modelo certo?&quot;)

for(i in 1:500){
  analise[i,1]&lt;- mean(coeficientes[2:11,i] != 0)
  analise[i,2]&lt;- mean(coeficientes[12:51,i] == 0)
  analise[i,3]&lt;- ifelse(analise[i,1]+analise[i,2] == 2,1,0)
}

tabela_final &lt;- colMeans(analise)*100
knitr::kable(tabela_final,caption = &quot;Os valores estão em porcentagem&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-1">Table 1: </span>Os valores estão em porcentagem</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Não zeros certos</td>
<td align="right">100.00</td>
</tr>
<tr class="even">
<td>Zeros certos</td>
<td align="right">84.29</td>
</tr>
<tr class="odd">
<td>Modelo certo?</td>
<td align="right">0.80</td>
</tr>
</tbody>
</table>
<p>O LASSO sempre inclui as variáveis relevantes, e exclui as variáveis irrelevantes em 84% das vezes. Mesmo assim, a proporção de vezes que o LASSO consegue recuperar o modelo correto é um pocuo mais que 1%. Isso parece esquisito a primeira vez, mas lembre que recuperar o modelo certo envolve <em>acertar todas as relevantes e todas as irrelevantes</em>. Se tivermos 50 variáveis e 500 replicações e em toda replicação o LASSO colocar apenas umas variável irrelevante no modelo, nós teríamos 98% de zeros certos (<span class="math inline">\(49/50\)</span>) e exatamente 0 modelos certos.</p>
<p>Parte do problema é que o LASSO penaliza todos os coeficientes igualmente, usando o <span class="math inline">\(\lambda\)</span>. Nós esperaríamos que algumas variáveis sejam mais importantes que outras - e isso pode vir a priori ou ser dito pelos dados. O LASSO adaptativo (adaLASSO) adiciona pesos (<span class="math inline">\(\omega\)</span>) para cada uma das variáveis na penalidade. Logo o novo problema a ser resolvido é:</p>
<p><span class="math display">\[\hat{\beta}_{adaLASSO} \in \arg \min_{\beta} \displaystyle \sum_{i=1}^n (y_i - x_i \beta)^2 + \lambda \sum_{k=0}^p \frac{|\beta_k|}{\omega_k}\]</span></p>
<p>A única exigência desses pesos é que eles sejam positivos. Veja que nós temos um novo parâmetro a escolher, os pesos. Eis um algoritmo muito simples que gera os pesos baseado nos dados e usa o LASSO:</p>
<ol style="list-style-type: decimal">
<li>Estime o modelo usando LASSO. Guarde os coeficientes, que eu chamarei de <span class="math inline">\(\beta_{LASSO}\)</span></li>
<li>Defina $_k = </li>
<li>Estime o adaLASSO usando os pesos definidos em 2.</li>
</ol>
<p>A boa notícia é que o <em>glmnet</em> já nos oferece uma opção para colocar o peso, via o argumento <em>penalty.factor</em>. Nosso trabalho é basicamente reduzido a escrever umas duas linhas de código a mais: uma que faz o LASSO de “primeiro estágio” e outra que define os pesos.</p>
<p>Uma coisa deve ficar evidente: da maneira que os pesos foram estabelecidos no meu algoritmo, coeficientes que são zerados pelo LASSO serão automaticamente excluídos pelo LASSO adaptativo: teremos como peso <span class="math inline">\(1/0\)</span> (com perdão aos matemáticos), que no limite é infinito. Como o LASSO não excluiu as variáveis relevantes em nenhum caso, não vamos nos preocupar. Mas é possível imaginar situações em que o LASSO poderia ter problemas (pense em coeficientes altissimamente correlacionados em uma amostra relativamente pequena). Outra coisa que deve ficar clara é que precisamos selecionar o <span class="math inline">\(\lambda\)</span> e agora duas vezes!</p>
<p>Vamos repetir a simulação ali de cima, mas usando o adaLASSO. EM ambos os estágios eu vou usar o Cross Validation:</p>
<pre class="r"><code>coeficientes_adalasso &lt;- Matrix(0,ncol=500,nrow=51)

for(i in 1:500){
  x &lt;- matrix(rnorm(50*100),ncol = 50) #gerando x
  betas &lt;- c(rep(1,10),rep(0,40)) #
  y &lt;- x%*%betas + rnorm(100)
  primeiro_estagio &lt;- cv.glmnet(x,y) #estimando usando LASSO e Cross Validation
  pesos &lt;- 1/abs(coef(primeiro_estagio)[-1,]) #veja que eu tenho que jogar fora o intercepto
  adalasso &lt;- cv.glmnet(x,y,penalty.factor = pesos)
  coeficientes_adalasso[,i] &lt;- coef(adalasso)
}

#Fim da simulação

analise &lt;- matrix(0,ncol=3,nrow=500)
colnames(analise)&lt;- c(&quot;Não zeros certos&quot;,&quot;Zeros certos&quot;, &quot;Modelo certo?&quot;)

for(i in 1:500){
  analise[i,1]&lt;- mean(coeficientes_adalasso[2:11,i] != 0)
  analise[i,2]&lt;- mean(coeficientes_adalasso[12:51,i] == 0)
  analise[i,3]&lt;- ifelse(analise[i,1]+analise[i,2] == 2,1,0)
}

tabela_final &lt;- colMeans(analise)*100
knitr::kable(tabela_final,caption = &quot;Os valores estão em porcentagem&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-2">Table 2: </span>Os valores estão em porcentagem</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Não zeros certos</td>
<td align="right">100.000</td>
</tr>
<tr class="even">
<td>Zeros certos</td>
<td align="right">97.105</td>
</tr>
<tr class="odd">
<td>Modelo certo?</td>
<td align="right">61.000</td>
</tr>
</tbody>
</table>
<p>A performance do LASSO adaptativo é muito melhor que a do LASSO: em 63% dos casos agora recuperamos o modelo correto, contra um pouco mais de 1% dos casos para o LASSO.</p>
