---
title: Concentração de Medida
author: Daniel Coutinho
date: '2020-11-14'
slug: concentração-de-medida.
categories:
  - Econometria
  - Alta Dimensão
tags:
  - Econometria
  - Estatística
images: []
authors: ["danielc"]
draft: true
---

<script src="2020-11-14-concentração-de-medida_files/header-attrs/header-attrs.js"></script>


<p>Este é um post meio maluco, porque é absolutamente específico a coisas de <em>machine learning</em> e não é nem de perto uma aplicação prática. Mas é um tópico que é muito interessante e que eu acho que é bastante acessível.</p>
<p>Vamos começar com um velho conhecido, a desigualdade de Chebyschev:</p>
<p><span class="math display">\[P(|X-\mu| &gt; t) \leq \frac{Var(x)}{t^2}\]</span></p>
<p>Nós todos conhecemos essa desigualdade e ela é usada como uma maneira de “provar” que a média é um estimador consistente: a variância da média de um processo i.i.d. com variância <span class="math inline">\(\sigma^2\)</span> é <span class="math inline">\(\sigma^2/n\)</span> e portanto Chebyschev nos dá:</p>
<p><span class="math display">\[P(|\bar{X}-\mu| &gt; t) \leq \frac{\sigma^2}{nt^2}\]</span></p>
<p>A Desigualdade de Chebyschev é uma aplicação de um resultado mais elementar, a desigualdade de Markov, que diz que para qualquer variável aleatória <em>positiva</em>:</p>
<p><span class="math display">\[P(X &gt; t) \leq \frac{E(X)}{t}\]</span></p>
<p>Se você trabalhar com <span class="math inline">\((X-\mu)^2\)</span> e <span class="math inline">\(t^2\)</span>, você chega na desigualdade de Chebyschev.</p>
<p>Existem outras maneiras de deixar uma variável positiva, e uma bacana é usando <span class="math inline">\(e^X\)</span>. Eu vou assumir média zero e trabalhar com <span class="math inline">\(e^{\lambda{}t}\)</span> e <span class="math inline">\(e^{\lambda{}x}\)</span>:</p>
<p><span class="math display">\[P(X &gt; t) = P(e^{\lambda{}X} &gt; e^{\lambda{}t}) \leq \frac{E(e^{\lambda{}X})}{e^{\lambda{}t}}\]</span></p>
<p>Agora, alguns de vocês sabem, mas apenas para garantir que todo mundo tá no mesmo lugar: <span class="math inline">\(E(e^{\lambda{}X})\)</span> é conhecido como a função geratriz de momentos. Nem todas as distribuições tem função geratriz de momentos. A parte bacana é que as derivadas com respeito a <span class="math inline">\(\lambda\)</span> da função geratriz de momentos (avaliada em <span class="math inline">\(\lambda=0\)</span>) nos dão os momentos da distribuição. Por exemplo, a normal com média <span class="math inline">\(\mu\)</span> e variância <span class="math inline">\(\sigma^2\)</span> tem como função geratriz de momentos <span class="math inline">\(e^{\lambda\mu + \lambda^2 \sigma^2/2}\)</span>. Tire as derivadas e avalie em <span class="math inline">\(\lambda=0\)</span> para se convencer de que o que eu disse é verdade.</p>
<p>Sabendo disso, nós podemos dizer que se <span class="math inline">\(X\)</span> é Normal com média zero e variância <span class="math inline">\(\sigma^2\)</span>, então a desigualdade que eu coloquei com exponencial implica:</p>
<p><span class="math display">\[P(X &gt; t) \leq e^{\lambda^2\sigma^2/2 - \lambda{}t}\]</span></p>
<p>Isso é verdade para qualquer <span class="math inline">\(\lambda\)</span>. A gente pode escolher o <span class="math inline">\(\lambda\)</span> que minimiza o valor na desigualdade, via as condições de primeira ordem, que nos dão:</p>
<p><span class="math display">\[\lambda\sigma^2 - t = 0 \therefore \lambda =  \frac{t}{\sigma^2}\]</span></p>
<p>Onde eu usei que <span class="math inline">\(\log\)</span> é monotônico e portanto não altera o ponto do máximo. Devolvendo isso para a desigualdade nós temos:</p>
<p><span class="math display">\[P(X &gt; t) \leq \exp\left(\frac{t^2\sigma^2}{2(\sigma^2)^2} - \frac{t^2}{\sigma^2}\right) = e^{-t^2/(2\sigma^2)}\]</span>
Essa desigualdade também tem um nome: Desigualdade de Chernoff (pra ser honesto: tudo que eu li disso é em inglês. Em inglês chamam de <em>Chernoff Bound</em>, mas eu traduzi para desigualdade já que temos a desigualdade de Chebyschev)</p>
<p>Deixa eu convencer vocês que isso é bem legal: eu vou simular 10 mil sorteios da normal padrão e comparar o que cada uma dessas desigualdades significa. Eu vou contar a probabilidade de ser maior que <span class="math inline">\(t\)</span> simplesmente computando a frequência em que os valores simulados são maiores que <span class="math inline">\(t\)</span> para um grid de valores <span class="math inline">\(t\)</span>. Eu vou trabalhar com o valor absoluto de <span class="math inline">\(X\)</span>:</p>
<pre class="r"><code>library(ggplot2) #gráficos bonitos
library(purrr) #pra computar o valor em cada um dos pontos do grid sem usar um for
library(tidyr) #pros dados ficarem do formato certo pro ggplot

n &lt;-10000 #tamanho da amostra
grid_size &lt;- 50 #tamanho do grid

tt &lt;- as.list(seq(0.1,2,length.out = grid_size)) #o grid

yy &lt;- rnorm(n)

freq &lt;- map_dbl(tt,~(sum(abs(yy) &gt; .))/n)

tt &lt;- do.call(c,tt)

df &lt;- data.frame(&quot;grid&quot; = tt,&quot;Empiric&quot; = freq,&quot;Chernoff&quot; = 2*exp(-tt^2/2),&quot;Chebyschev&quot; = 1/tt^2)

df2 &lt;- pivot_longer(df,cols = c(&quot;Empiric&quot;,&quot;Chernoff&quot;,&quot;Chebyschev&quot;))

ggplot(df2,aes(grid,value,color = name)) + geom_line()</code></pre>
<p><img src="/post/2020-11-14-concentra%C3%A7%C3%A3o-de-medida_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Veja que eu fiz um <em>hardcode</em> de que a variância é 1. Fica bem claro que:</p>
<ol style="list-style-type: decimal">
<li><p>As duas desigualdades fazem o que prometem: elas estão sempre acima na probabilidade verdadeira</p></li>
<li><p>Chebyschev distorce totalmente o gráfico porque é extremamente generosa</p></li>
</ol>
<p>Vamos eliminar Chebyschev do gráfico:</p>
<pre class="r"><code>df3 &lt;-df[,-4]

df3 &lt;- pivot_longer(df3,cols = c(&quot;Empiric&quot;,&quot;Chernoff&quot;))

ggplot(df3,aes(grid,value,color = name)) + geom_line()</code></pre>
<p><img src="/post/2020-11-14-concentra%C3%A7%C3%A3o-de-medida_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Veja que essa cota é bem generosa, mas bem melhor que do que a desigualdade de Chebyschev. Obviamente, pagamos um preço por isso: nem todas as funções com variância finita tem função geratriz de momentos.</p>
<p>Um fato mais interessante é que você pode usar a desigualdade de Chernoff, exatamente como posta acima, para <em>outras distribuições além da gaussiana</em>. Basicamente, você está exigindo que a cauda da distribuição caia <em>tão rápido quanto a gaussiana</em>. Distribuições que atendem a esse requisito são chamadas de subgaussianas. Todas as distribuições com suporte finito atendem a esse pré requisito, por exemplo. Por sinal, <span class="math inline">\(\sigma\)</span> no caso mais geral não precisa ser a variância da distribuição. Nem todas as distribuições são subgaussianas: a qui-quadrado não é sub gaussiana, e é uma distribuição extremamente comum em estatística.</p>
<p>Veja que essa cota depende exponencialmente de <span class="math inline">\(t^2\)</span>, ao contrário de Chebyschev que depende de <span class="math inline">\(1/t^2\)</span>. Isso é um ganho dramático: com altíssima probabilidade a massa da distribuição está concentrada. Além de ser bem bacana, isto aparece em várias situações em <em>machine learning</em>.</p>
