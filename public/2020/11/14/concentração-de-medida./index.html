<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<title>
  
     Concentração de Medida | 
    AZUL
  
</title><meta name="description" content="Economia, Estatística, Programação"><meta name="author" content="danielc">

<link rel="apple-touch-icon" href="/apple-touch-icon.png" sizes="180x180">
<link rel="icon" href="/favicon-32x32.png " sizes="32x32" type="image/png">
<link rel="icon" href="/favicon-16x16.png" sizes="16x16" type="image/png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#0c344b">
<link rel="icon" href="/favicon.ico">

//styles, look here: https://cdnjs.com/libraries/highlight.js/9.12.0

<link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/vs2015.min.css" rel="stylesheet">





    
        
            <link rel="stylesheet" href="/dist/main.37ab3f61b95417873748.min.css">
        
    




<link rel="canonical" href="/2020/11/14/concentra%C3%A7%C3%A3o-de-medida./"><meta property="og:title" content="Concentração de Medida" />
<meta property="og:description" content="Este é um post meio maluco, porque é absolutamente específico a coisas de machine learning e não é nem de perto uma aplicação prática. Mas é um tópico que é muito interessante e que eu acho que é bastante acessível.
Vamos começar com um velho conhecido, a desigualdade de Chebyschev:
\[P(|X-\mu| &gt; t) \leq \frac{Var(x)}{t^2}\]
Nós todos conhecemos essa desigualdade e ela é usada como uma maneira de “provar” que a média é um estimador consistente: a variância da média de um processo i." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/2020/11/14/concentra%C3%A7%C3%A3o-de-medida./" />
<meta property="article:published_time" content="2020-11-14T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-11-14T00:00:00+00:00" />
<meta itemprop="name" content="Concentração de Medida">
<meta itemprop="description" content="Este é um post meio maluco, porque é absolutamente específico a coisas de machine learning e não é nem de perto uma aplicação prática. Mas é um tópico que é muito interessante e que eu acho que é bastante acessível.
Vamos começar com um velho conhecido, a desigualdade de Chebyschev:
\[P(|X-\mu| &gt; t) \leq \frac{Var(x)}{t^2}\]
Nós todos conhecemos essa desigualdade e ela é usada como uma maneira de “provar” que a média é um estimador consistente: a variância da média de um processo i.">
<meta itemprop="datePublished" content="2020-11-14T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-11-14T00:00:00+00:00" />
<meta itemprop="wordCount" content="781">



<meta itemprop="keywords" content="Econometria,Estatística," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Concentração de Medida"/>
<meta name="twitter:description" content="Este é um post meio maluco, porque é absolutamente específico a coisas de machine learning e não é nem de perto uma aplicação prática. Mas é um tópico que é muito interessante e que eu acho que é bastante acessível.
Vamos começar com um velho conhecido, a desigualdade de Chebyschev:
\[P(|X-\mu| &gt; t) \leq \frac{Var(x)}{t^2}\]
Nós todos conhecemos essa desigualdade e ela é usada como uma maneira de “provar” que a média é um estimador consistente: a variância da média de um processo i."/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>



</head>
<body>
    
<nav class="navbar navbar-expand-md navbar-light bg-light fixed-top shadow-sm" id="navbar-main-menu">
    <div class="container">
        <a class="navbar-brand font-weight-bold" href="/">AZUL</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-menu" aria-controls="main-menu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="main-menu">
            <ul class="navbar-nav ml-auto">
                
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/categories/">Categorias</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/about/">Sobre</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/tags/">Tags</a></li>
                
            
            </ul>
        </div>
    </div>
</nav>


    
<main class="content-page container pt-7 pb-5">
    
    <div class="row">
        <div class="col">
            <article>
                <div class="row justify-content-center">
                    <div class="col-lg-8">
                        <div class="meta text-muted mb-3">
                            <p class="created text-muted text-uppercase font-weight-bold mb-1">November 14, 2020</p>
                            <span class="mr-2"><i class="fas fa-book-open mr-2"></i>781 palavras</span>
                            <span><i class="fas fa-clock mr-2"></i>4 mins</span>
                        </div>

                        <h1>Concentração de Medida</h1>

                        <ul class="authors list-inline"><li class="list-inline-item mr-3">
                    <div class="media author"><div class="media-body">
                            <h5 class="name my-0"><a href="/authors/danielc/" class="small">Daniel Coutinho</a>
                            </h5></div>
                    </div>
                </li></ul>
                    </div>
                </div><div class="row justify-content-center">
                    <div class="col-lg-8">
                        <div class="content">
                            
<script src="2020-11-14-concentração-de-medida_files/header-attrs/header-attrs.js"></script>


<p>Este é um post meio maluco, porque é absolutamente específico a coisas de <em>machine learning</em> e não é nem de perto uma aplicação prática. Mas é um tópico que é muito interessante e que eu acho que é bastante acessível.</p>
<p>Vamos começar com um velho conhecido, a desigualdade de Chebyschev:</p>
<p><span class="math display">\[P(|X-\mu| &gt; t) \leq \frac{Var(x)}{t^2}\]</span></p>
<p>Nós todos conhecemos essa desigualdade e ela é usada como uma maneira de “provar” que a média é um estimador consistente: a variância da média de um processo i.i.d. com variância <span class="math inline">\(\sigma^2\)</span> é <span class="math inline">\(\sigma^2/n\)</span> e portanto Chebyschev nos dá:</p>
<p><span class="math display">\[P(|\bar{X}-\mu| &gt; t) \leq \frac{\sigma^2}{nt^2}\]</span></p>
<p>A Desigualdade de Chebyschev é uma aplicação de um resultado mais elementar, a desigualdade de Markov, que diz que para qualquer variável aleatória <em>positiva</em>:</p>
<p><span class="math display">\[P(X &gt; t) \leq \frac{E(X)}{t}\]</span></p>
<p>Se você trabalhar com <span class="math inline">\((X-\mu)^2\)</span> e <span class="math inline">\(t^2\)</span>, você chega na desigualdade de Chebyschev.</p>
<p>Existem outras maneiras de deixar uma variável positiva, e uma bacana é usando <span class="math inline">\(e^X\)</span>. Eu vou assumir média zero e trabalhar com <span class="math inline">\(e^{\lambda{}t}\)</span> e <span class="math inline">\(e^{\lambda{}x}\)</span>:</p>
<p><span class="math display">\[P(X &gt; t) = P(e^{\lambda{}X} &gt; e^{\lambda{}t}) \leq \frac{E(e^{\lambda{}X})}{e^{\lambda{}t}}\]</span></p>
<p>Agora, alguns de vocês sabem, mas apenas para garantir que todo mundo tá no mesmo lugar: <span class="math inline">\(E(e^{\lambda{}X})\)</span> é conhecido como a função geratriz de momentos. Nem todas as distribuições tem função geratriz de momentos. A parte bacana é que as derivadas com respeito a <span class="math inline">\(\lambda\)</span> da função geratriz de momentos (avaliada em <span class="math inline">\(\lambda=0\)</span>) nos dão os momentos da distribuição. Por exemplo, a normal com média <span class="math inline">\(\mu\)</span> e variância <span class="math inline">\(\sigma^2\)</span> tem como função geratriz de momentos <span class="math inline">\(e^{\lambda\mu + \lambda^2 \sigma^2/2}\)</span>. Tire as derivadas e avalie em <span class="math inline">\(\lambda=0\)</span> para se convencer de que o que eu disse é verdade.</p>
<p>Sabendo disso, nós podemos dizer que se <span class="math inline">\(X\)</span> é Normal com média zero e variância <span class="math inline">\(\sigma^2\)</span>, então a desigualdade que eu coloquei com exponencial implica:</p>
<p><span class="math display">\[P(X &gt; t) \leq e^{\lambda^2\sigma^2/2 - \lambda{}t}\]</span></p>
<p>Isso é verdade para qualquer <span class="math inline">\(\lambda\)</span>. A gente pode escolher o <span class="math inline">\(\lambda\)</span> que minimiza o valor na desigualdade, via as condições de primeira ordem, que nos dão:</p>
<p><span class="math display">\[\lambda\sigma^2 - t = 0 \therefore \lambda =  \frac{t}{\sigma^2}\]</span></p>
<p>Onde eu usei que <span class="math inline">\(\log\)</span> é monotônico e portanto não altera o ponto do máximo. Devolvendo isso para a desigualdade nós temos:</p>
<p><span class="math display">\[P(X &gt; t) \leq \exp\left(\frac{t^2\sigma^2}{2(\sigma^2)^2} - \frac{t^2}{\sigma^2}\right) = e^{-t^2/(2\sigma^2)}\]</span>
Essa desigualdade também tem um nome: Desigualdade de Chernoff (pra ser honesto: tudo que eu li disso é em inglês. Em inglês chamam de <em>Chernoff Bound</em>, mas eu traduzi para desigualdade já que temos a desigualdade de Chebyschev)</p>
<p>Deixa eu convencer vocês que isso é bem legal: eu vou simular 10 mil sorteios da normal padrão e comparar o que cada uma dessas desigualdades significa. Eu vou contar a probabilidade de ser maior que <span class="math inline">\(t\)</span> simplesmente computando a frequência em que os valores simulados são maiores que <span class="math inline">\(t\)</span> para um grid de valores <span class="math inline">\(t\)</span>. Eu vou trabalhar com o valor absoluto de <span class="math inline">\(X\)</span>:</p>
<pre class="r"><code>library(ggplot2) #gráficos bonitos
library(purrr) #pra computar o valor em cada um dos pontos do grid sem usar um for
library(tidyr) #pros dados ficarem do formato certo pro ggplot

n &lt;-10000 #tamanho da amostra
grid_size &lt;- 50 #tamanho do grid

tt &lt;- as.list(seq(0.1,2,length.out = grid_size)) #o grid

yy &lt;- rnorm(n)

freq &lt;- map_dbl(tt,~(sum(abs(yy) &gt; .))/n)

tt &lt;- do.call(c,tt)

df &lt;- data.frame(&quot;grid&quot; = tt,&quot;Empiric&quot; = freq,&quot;Chernoff&quot; = 2*exp(-tt^2/2),&quot;Chebyschev&quot; = 1/tt^2)

df2 &lt;- pivot_longer(df,cols = c(&quot;Empiric&quot;,&quot;Chernoff&quot;,&quot;Chebyschev&quot;))

ggplot(df2,aes(grid,value,color = name)) + geom_line()</code></pre>
<p><img src="/post/2020-11-14-concentra%C3%A7%C3%A3o-de-medida_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Veja que eu fiz um <em>hardcode</em> de que a variância é 1. Fica bem claro que:</p>
<ol style="list-style-type: decimal">
<li><p>As duas desigualdades fazem o que prometem: elas estão sempre acima na probabilidade verdadeira</p></li>
<li><p>Chebyschev distorce totalmente o gráfico porque é extremamente generosa</p></li>
</ol>
<p>Vamos eliminar Chebyschev do gráfico:</p>
<pre class="r"><code>df3 &lt;-df[,-4]

df3 &lt;- pivot_longer(df3,cols = c(&quot;Empiric&quot;,&quot;Chernoff&quot;))

ggplot(df3,aes(grid,value,color = name)) + geom_line()</code></pre>
<p><img src="/post/2020-11-14-concentra%C3%A7%C3%A3o-de-medida_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Veja que essa cota é bem generosa, mas bem melhor que do que a desigualdade de Chebyschev. Obviamente, pagamos um preço por isso: nem todas as funções com variância finita tem função geratriz de momentos.</p>
<p>Um fato mais interessante é que você pode usar a desigualdade de Chernoff, exatamente como posta acima, para <em>outras distribuições além da gaussiana</em>. Basicamente, você está exigindo que a cauda da distribuição caia <em>tão rápido quanto a gaussiana</em>. Distribuições que atendem a esse requisito são chamadas de subgaussianas. Todas as distribuições com suporte finito atendem a esse pré requisito, por exemplo. Por sinal, <span class="math inline">\(\sigma\)</span> no caso mais geral não precisa ser a variância da distribuição. Nem todas as distribuições são subgaussianas: a qui-quadrado não é sub gaussiana, e é uma distribuição extremamente comum em estatística.</p>
<p>Veja que essa cota depende exponencialmente de <span class="math inline">\(t^2\)</span>, ao contrário de Chebyschev que depende de <span class="math inline">\(1/t^2\)</span>. Isso é um ganho dramático: com altíssima probabilidade a massa da distribuição está concentrada. Além de ser bem bacana, isto aparece em várias situações em <em>machine learning</em>.</p>

                        </div><div class="tags my-3"><a class="badge badge-pill badge-light border mr-2" href="/tags/econometria">
                                    <i class="fas fa-tag mr-2"></i>Econometria
                                </a><a class="badge badge-pill badge-light border mr-2" href="/tags/estat%C3%ADstica">
                                    <i class="fas fa-tag mr-2"></i>Estatística
                                </a></div><ul class="share nav my-3 justify-content-end">
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://twitter.com/intent/tweet?url=%2f2020%2f11%2f14%2fconcentra%25C3%25A7%25C3%25A3o-de-medida.%2f&text=Concentra%c3%a7%c3%a3o%20de%20Medida">
              <i class="fa-fw fab fa-twitter"></i>
          </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://www.linkedin.com/shareArticle?url=%2f2020%2f11%2f14%2fconcentra%25C3%25A7%25C3%25A3o-de-medida.%2f&title=Concentra%c3%a7%c3%a3o%20de%20Medida">
                <i class="fa-fw fab fa-linkedin-in"></i>
            </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://www.facebook.com/sharer.php?u=%2f2020%2f11%2f14%2fconcentra%25C3%25A7%25C3%25A3o-de-medida.%2f&t=Concentra%c3%a7%c3%a3o%20de%20Medida">
                <i class="fa-fw fab fa-facebook-f"></i>
            </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://reddit.com/submit?url=%2f2020%2f11%2f14%2fconcentra%25C3%25A7%25C3%25A3o-de-medida.%2f&title=Concentra%c3%a7%c3%a3o%20de%20Medida">
                <i class="fa-fw fab fa-reddit-alien"></i>
            </a>
        </li>
    </nav>
                    </div>
                </div>

                <div class="row justify-content-center">
                    <div class="col-lg-8">
                        
                    </div>
                </div></article>
        </div>
    </div>

    <div class="related-content row mt-5 row-cols-1 row-cols-lg-3"><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/2020/10/10/verossimilhan%C3%A7a-da-poisson/" class="d-block"><div class="card-body">
            <h4 class="card-title">Verossimilhança da Poisson</h4>
            <p class="card-text text-muted text-uppercase">October 10, 2020</p>
            <div class="card-text">
                Dislaimer: eu tenho a formação em estatística de uma batata, não me leve muito a sério
A distribuição de Poisson descreve a probabilidade de que \(k\) eventos discretos ocorram em um espaço ou período de tempo em que \(\lambda\) eventos eram esperados. A densidade é:
\[f(k \, | \,\lambda) = \frac{\lambda^k e^{-\lambda}}{k!}\]
Existe um bom número de situações em que essa distribuição pode não ser adequada para modelar o que se propõe, mas vamos passar por cima disso.
            </div>
        </div>
    </a>
</div>

            </div><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/2020/06/02/modelagem-com-tidymodels/" class="d-block"><div class="card-body">
            <h4 class="card-title">Modelagem com {tidymodels}</h4>
            <p class="card-text text-muted text-uppercase">June 2, 2020</p>
            <div class="card-text">
                
            </div>
        </div>
    </a>
</div>

            </div><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/2020/02/09/difusao-gaussiana/" class="d-block"><div class="card-body">
            <h4 class="card-title">Gerando um padrão de difusão com soma de um termo gaussiano</h4>
            <p class="card-text text-muted text-uppercase">February 9, 2020</p>
            <div class="card-text">
                Dia desses eu fui informado por um amigo de que um padrão bonitinho de difusão acontece somando um termo gaussiano acumuladamente a um conjunto. O que o amigo versado em Física me relatou como um “padrão de difusão”, na minha intuição mais econométrica vem como uma random walk no \(\mathbb{R}^2\).
Bem, vamos usar o purrr e o dplyr para gerar de maneira concisa um tibble pronto para ser passado ao ggplot2.
            </div>
        </div>
    </a>
</div>

            </div></div>
</main>


    <footer class="footer text-center bg-dark py-6">
    <div class="container">
        <div class="row">
            <div class="col">
                <ul class="list-inline">
                    <li class="list-inline-item"><a href="/index.xml" rel="alternate" type="application/rss+xml" class="icons d-block">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a></li><li class="list-inline-item">
                            <a href="https://github.com/danmrc/azul/tree/master/C%C3%B3digos" class="icons d-block">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                </ul>

                <p class="text-muted">
                    
                        Copyright © 2008–2020, Pedro Cavalcante & Daniel Coutinho; all rights reserved.
                    
                </p>

                <p class="text-muted">
                Powered by <a href="https://gohugo.io" target="_blank">Hugo</a> with <a href="https://github.com/puresyntax71/hugo-theme-chunky-poster" target="_blank">Chunky Poster</a>.
                </p>
            </div>
        </div>
    </div>
</footer>

    
    
        
            <script src="/dist/main.d608eadfe5ac0688902e.min.js"></script>
        
    






<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>

<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-123754589-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

</body>
</html>
