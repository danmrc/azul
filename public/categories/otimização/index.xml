<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Otimização on AZUL</title>
    <link>https://azul.netlify.app/categories/otimiza%C3%A7%C3%A3o/</link>
    <description>Recent content in Otimização on AZUL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <copyright>Copyright © 2008–2020, Pedro Cavalcante &amp; Daniel Coutinho; all rights reserved.</copyright>
    <lastBuildDate>Tue, 21 Apr 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://azul.netlify.app/categories/otimiza%C3%A7%C3%A3o/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Jogo da Velha com Q-Learning</title>
      <link>https://azul.netlify.app/2020/04/21/jogo-da-velha-com-q-learning/</link>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://azul.netlify.app/2020/04/21/jogo-da-velha-com-q-learning/</guid>
      <description>Aqui no blog já abordamos várias vezes técnicas que podemos colocar na caixinha do Aprendizado Supervisionado - onde praticamente todo o ferramental da Econometria está. Também abordamos Aprendizado Não-Supervisionado quando falamos de clustering k-means. Acho que vale agora por o dedinho na água do Aprendizado por Reforço. Deixo o aviso de que apesar de falarmos que abordamos o conteúdo aqui da maneira como gostaríamos de ao assunto ter sido apresentados, definitivamente não é assim que eu gostaria de ter sido introduzido a Aprendizado por Reforço porque, bem, eu não fui introduzido a esse mundo, não de verdade.</description>
    </item>
    
  </channel>
</rss>
