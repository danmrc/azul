<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<title>
  
     O LASSO | 
    AZUL
  
</title><meta name="description" content="Economia, Estatística, Programação"><meta name="author" content="danielc">

<link rel="apple-touch-icon" href="/apple-touch-icon.png" sizes="180x180">
<link rel="icon" href="/favicon-32x32.png " sizes="32x32" type="image/png">
<link rel="icon" href="/favicon-16x16.png" sizes="16x16" type="image/png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#0c344b">
<link rel="icon" href="/favicon.ico">

//styles, look here: https://cdnjs.com/libraries/highlight.js/9.12.0

<link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/vs2015.min.css" rel="stylesheet">





    
        
            <link rel="stylesheet" href="/dist/main.37ab3f61b95417873748.min.css">
        
    




<link rel="canonical" href="/2018/09/16/lasso/"><meta property="og:title" content="O LASSO" />
<meta property="og:description" content="Este post vai tratar de um método de machine learning muito interessante e relativamente simples: o LASSO. LASSO significa Least Absolute Shrinkage and Select Operator. Como o nome sugere, o LASSO seleciona quais regressores são relevantes e quais não são. Ou seja, suponha que você é um pesquisador que tem 50 variáveis que são possíveis candidatos a variáveis explicativas de uma variável de interesse. O LASSO permite que você dê os 50 regressores para o computador e ele escolha quais são os relevantes." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/2018/09/16/lasso/" />
<meta property="article:published_time" content="2018-09-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-09-16T00:00:00+00:00" />
<meta itemprop="name" content="O LASSO">
<meta itemprop="description" content="Este post vai tratar de um método de machine learning muito interessante e relativamente simples: o LASSO. LASSO significa Least Absolute Shrinkage and Select Operator. Como o nome sugere, o LASSO seleciona quais regressores são relevantes e quais não são. Ou seja, suponha que você é um pesquisador que tem 50 variáveis que são possíveis candidatos a variáveis explicativas de uma variável de interesse. O LASSO permite que você dê os 50 regressores para o computador e ele escolha quais são os relevantes.">
<meta itemprop="datePublished" content="2018-09-16T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2018-09-16T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1823">



<meta itemprop="keywords" content="LASSO,Machine Learning,R," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="O LASSO"/>
<meta name="twitter:description" content="Este post vai tratar de um método de machine learning muito interessante e relativamente simples: o LASSO. LASSO significa Least Absolute Shrinkage and Select Operator. Como o nome sugere, o LASSO seleciona quais regressores são relevantes e quais não são. Ou seja, suponha que você é um pesquisador que tem 50 variáveis que são possíveis candidatos a variáveis explicativas de uma variável de interesse. O LASSO permite que você dê os 50 regressores para o computador e ele escolha quais são os relevantes."/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>



</head>
<body>
    
<nav class="navbar navbar-expand-md navbar-light bg-light fixed-top shadow-sm" id="navbar-main-menu">
    <div class="container">
        <a class="navbar-brand font-weight-bold" href="/">AZUL</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-menu" aria-controls="main-menu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="main-menu">
            <ul class="navbar-nav ml-auto">
                
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/categories/">Categorias</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/about/">Sobre</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/tags/">Tags</a></li>
                
            
            </ul>
        </div>
    </div>
</nav>


    
<main class="content-page container pt-7 pb-5">
    
    <div class="row">
        <div class="col">
            <article>
                <div class="row justify-content-center">
                    <div class="col-lg-8">
                        <div class="meta text-muted mb-3">
                            <p class="created text-muted text-uppercase font-weight-bold mb-1">September 16, 2018</p>
                            <span class="mr-2"><i class="fas fa-book-open mr-2"></i>1823 palavras</span>
                            <span><i class="fas fa-clock mr-2"></i>9 mins</span>
                        </div>

                        <h1>O LASSO</h1>

                        <ul class="authors list-inline"><li class="list-inline-item mr-3">
                    <div class="media author"><div class="media-body">
                            <h5 class="name my-0"><a href="/authors/danielc/" class="small">Daniel Coutinho</a>
                            </h5></div>
                    </div>
                </li></ul>
                    </div>
                </div><div class="row justify-content-center">
                    <div class="col-lg-8">
                        <div class="content">
                            


<p>Este post vai tratar de um método de machine learning muito interessante e relativamente simples: o LASSO. LASSO significa <em>Least Absolute Shrinkage and Select Operator</em>. Como o nome sugere, o LASSO <strong>seleciona quais regressores são relevantes e quais não são</strong>. Ou seja, suponha que você é um pesquisador que tem 50 variáveis que são possíveis candidatos a variáveis explicativas de uma variável de interesse. O LASSO permite que você dê os 50 regressores para o computador e ele escolha quais são os relevantes. O LASSO está centrado na ideia de <em>esparsidade</em>: poucos coeficientes vão ser diferentes de zero; poucas variáveis vão ser relevantes numa regressão.</p>
<p>Uma pergunta justa é: por que não simplesmente testamos isso “na mão”? Isso é, fazemos uma regressão com todas as variáveis, uma sem a primeira variável da base de dados, uma outra sem a segunda… e no fim vemos qual a melhor regressão? Além de problemas de teste - lembre que todo teste tem uma chance de você estar tomando a decisão <em>errada</em> - isso seria um inferno: com relés 50 variáveis, precisaríamos de <span class="math inline">\(2^{50}\)</span> regressões, ou seja, 1.125899910^{15} regressões: esse é um número com 15 zeros.</p>
<p>A ideia original vem de um paper de 1996 de Robert Tibshirani. Não temos solução fechada para encontrar o estimador de LASSO, ao contrário do MQO, onde podemos expressar o estimador como uma multiplicação de matrizes. Portanto, é um estimador que seria impossível sem um computador. Mas a ideia é incrivelmente simples.</p>
<p>Pegue o modelo usual de regressão, <span class="math inline">\(y_i = X_i \beta + u_i\)</span>, onde <span class="math inline">\(\beta\)</span> é o coeficiente de interesse, <span class="math inline">\(u\)</span> é um choque aleatório, as observações são indexadas por <span class="math inline">\(i = 1,...,n\)</span> e os coeficientes são indexados por <span class="math inline">\(k = 1,..,K\)</span> . O objetivo do MQO é minimizar a soma dos quadrados dos resíduos, ou seja <span class="math inline">\(\min_{\beta} \sum_{i=1}^n \hat{u}^2\)</span>. O LASSO começa dai, mas adiciona uma pequena alteração: vamos limitar o valor da soma dos valores absolutos dos coeficientes para que ele seja menor que uma constante <span class="math inline">\(c\)</span>. Ou seja, o LASSO resolve o problema:</p>
<p><span class="math display">\[\min_{\beta} \sum_{i=1}^n \hat{u}^2  \text{ sujeito a }  \sum_{k=1}^K |\beta_{k}| &lt; c\]</span></p>
<p>Veja que não temos como tirar a derivada da função módulo, e portanto não podemos resolver o problema “no braço”.</p>
<p>Podemos reescrever o problema como um la grangeano: <span class="math inline">\(\min_{\beta} \left( \sum_{i=1}^n \hat{u}^2 \right) - \lambda \left( \sum_{k=1}^K |\beta_{k}| \right)\)</span>, onde o <span class="math inline">\(\lambda\)</span> é o multiplicador de lagrange. Existe uma função que leva de <span class="math inline">\(c\)</span> para <span class="math inline">\(\lambda\)</span>, então podemos ignorar o valor de <span class="math inline">\(c\)</span> e pensar só em termos de <span class="math inline">\(\lambda\)</span>. Muitas implementações fazem isso e eu seguirei este caminho.</p>
<p>Veja que estamos trabalhando a soma do valor absoluto dos coeficientes. Chamamos isso de norma <span class="math inline">\(\ell_1\)</span>. Aqueles que já fizeram um curso de algebra linear conhecem a norma <span class="math inline">\(\ell_2\)</span>, conhecida como norma euclidiana: <span class="math inline">\(\sum_{k=1}^K \beta{}^2\)</span>. Existe um método de estimação que usa a norma <span class="math inline">\(\ell_2\)</span> ao invés da <span class="math inline">\(\ell_1\)</span>, conhecido como <em>ridge</em>. Usando a norma <span class="math inline">\(\ell_2\)</span>, o problema tem solução analítica. Então, por que norma <span class="math inline">\(\ell_1\)</span>, que parece ser tão ruim de trabalhar?</p>
<p>Esta é uma das belezas do LASSO: a norma <span class="math inline">\(\ell_2\)</span> <strong>não</strong> gera esparsidade. A norma <span class="math inline">\(\ell_1\)</span> gera. O motivo é ilustrado na figura abaixo, tirada de <em>Statistical Learning with Sparsity</em>, de Trevor Hastie, Robert Tibshirani and Martin Wainwright (cujo pdf, 100% legal, você encontra <a href="https://web.stanford.edu/~hastie/StatLearnSparsity_files/SLS_corrected_1.4.16.pdf">aqui</a>)</p>
<div class="figure">
<img src="/post/LASSO/aquela_classica_imagem_do_lasso.png" />

</div>
<p>A imagem supõe que você só tem duas variáveis, por motivos ilustrativos. O <span class="math inline">\(\hat{\beta}\)</span> é o máximo global, o conhecido estimador de MQO. Em vermelho são as curvas de nível do estimador. O que o <em>Ridge</em> (à direita) faz é impor uma restrição no formato de uma bola (azul). Veja que a curva de nível encosta na bola fora do eixo vertical - onde <span class="math inline">\(\beta_1\)</span> seria zero. Já o <em>LASSO</em> é ilustrado na figura à esquerda. Veja que, nesse caso, a curva de nível acerta na quina da restrição, efetivamente zerando o coeficiente <span class="math inline">\(\beta_1\)</span>.</p>
<p>Veja que, por enquanto, eu escondi o <span class="math inline">\(\lambda\)</span> (ou o <span class="math inline">\(c\)</span>) debaixo do tapete: eu ainda não expliquei como escolher essa variável, que de fato está no centro do problema. O pesquisador pode ter uma vaga ideia de quanto os coeficientes devem somar. Mas uma ideia exata do tamanho é complicada, então gostariamos de deixar os dados nos dizerem o <span class="math inline">\(\lambda\)</span> certo. Existem várias estratégias, e infelizmente o <code>glmnet</code> só implementa uma.</p>
<p>Com alguma ideia do que o LASSO está fazendo, podemos colocar a mão na massa e discutir o pacote padrão para o LASSO no R, o <code>glmnet</code>.</p>
<div id="o-pacote-glmnet" class="section level2">
<h2>O pacote glmnet</h2>
<p>Obviamente, começamos carregando o pacote:</p>
<pre class="r"><code>library(glmnet)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## Loaded glmnet 2.0-16</code></pre>
<p>O comando para rodar uma regressão usando o LASSO é o <code>glmnet</code>. Ele recebe basicamente duas coisas, uma matriz com regressores e o vetor que é a variável dependente. Até ai, muito parecido com o MQO. Mas a grande diferença em termos de output é que o <code>glmnet</code> devolve uma <em>matriz</em> de coeficientes. O que o <code>glmnet</code> faz é estimar o modelo para um vetor de <span class="math inline">\(\lambda\)</span>, de tal forma que você vá de nenhuma variável incluida no modelo até todas as variáveis. Isso não significa que cada <span class="math inline">\(\lambda\)</span> estimado adicione uma variável nova ao modelo: em alguns casos o LASSO não seleciona mais ninguém, mas como ele tem mais “orçamento” para alocar, ele aumenta o valor dos coeficientes das variáveis incluídas. O <code>glmnet</code> resolve o problema de escolher qual <span class="math inline">\(\lambda\)</span> usar simplesmente estimando com vários valores diferentes.</p>
<p>Vamos testar o <code>glmnet</code> gerando uma matriz com 50 variáveis e 2000 observações e criando uma variável dependente que depende só das 10 primeiras variáveis:</p>
<pre class="r"><code>x &lt;- matrix(rnorm(2000*50), ncol = 50) #regressores
betas &lt;- c(rep(1,10),rep(0,40)) # as 10 primeiras serão relevantes com um coeficiente 1. As outras são irrelevantes - e portanto tem um coeficiente 0
y &lt;- x%*%betas + rnorm(2000) 

modelo &lt;- glmnet(x,y)
coef(modelo)[,1:5] # só pegando as 5 primeiras colunas. Veja que o comando summary não nos dá o que queremos no glmnet</code></pre>
<pre><code>## 51 x 5 sparse Matrix of class &quot;dgCMatrix&quot;
##                      s0           s1          s2          s3          s4
## (Intercept) -0.02236141 -0.024213986 -0.02408282 -0.02267777 -0.02158887
## V1           .           0.006876862  0.09575011  0.17453072  0.24630317
## V2           .           .            0.06824857  0.15461215  0.23346785
## V3           .           .            0.08042146  0.16019408  0.23296031
## V4           .           .            0.03010058  0.11588392  0.19410939
## V5           .           .            0.03868786  0.12617288  0.20601426
## V6           .           .            0.01207862  0.09721205  0.17469666
## V7           .           0.099490703  0.18116146  0.25477479  0.32192524
## V8           .           0.087586725  0.17083515  0.24721020  0.31704986
## V9           .           .            .           0.08388819  0.16614762
## V10          .           .            0.05542081  0.14004438  0.21702939
## V11          .           .            .           .           .         
## V12          .           .            .           .           .         
## V13          .           .            .           .           .         
## V14          .           .            .           .           .         
## V15          .           .            .           .           .         
## V16          .           .            .           .           .         
## V17          .           .            .           .           .         
## V18          .           .            .           .           .         
## V19          .           .            .           .           .         
## V20          .           .            .           .           .         
## V21          .           .            .           .           .         
## V22          .           .            .           .           .         
## V23          .           .            .           .           .         
## V24          .           .            .           .           .         
## V25          .           .            .           .           .         
## V26          .           .            .           .           .         
## V27          .           .            .           .           .         
## V28          .           .            .           .           .         
## V29          .           .            .           .           .         
## V30          .           .            .           .           .         
## V31          .           .            .           .           .         
## V32          .           .            .           .           .         
## V33          .           .            .           .           .         
## V34          .           .            .           .           .         
## V35          .           .            .           .           .         
## V36          .           .            .           .           .         
## V37          .           .            .           .           .         
## V38          .           .            .           .           .         
## V39          .           .            .           .           .         
## V40          .           .            .           .           .         
## V41          .           .            .           .           .         
## V42          .           .            .           .           .         
## V43          .           .            .           .           .         
## V44          .           .            .           .           .         
## V45          .           .            .           .           .         
## V46          .           .            .           .           .         
## V47          .           .            .           .           .         
## V48          .           .            .           .           .         
## V49          .           .            .           .           .         
## V50          .           .            .           .           .</code></pre>
<p>Obviamente, ainda temos que selecionar qual dos modelos nós queremos. O pacote nos permite selecionar por <em>Cross Validation</em> (CV). Em linhas gerais, o que o Cross Validation faz é separar os dados em <span class="math inline">\(k\)</span> blocos - 5,10, ou até mesmo blocos de uma única observação (conhecido como <em>leave one out</em>). Depois, escolha <span class="math inline">\(k-1\)</span> blocos e estime o modelo usando o <code>glmnet</code>. Para avaliar qual <span class="math inline">\(\lambda\)</span> devemos usar, use o bloco que <em>não</em> foi usado para estimar para ver qual modelo gera o menor erro segundo alguma métrica (erro quadrático médio, por exemplo). O CV vai permitir com que cada bloco tenha uma chance de ser “fora da amostra”.</p>
<p>Isso pode parecer muito computacionalmente intensivo - e de fato é - mas a implementação do glmnet é tão eficiente que normalmente num piscar de olhos o modelo é estimado - no meu Dell Vostro de 2012 com i5, 6GB e Windows 10, o tempo é de apenas 0.19s. O comando que faz a seleção do <span class="math inline">\(\lambda\)</span> é o <code>cv.glmnet</code>. Vamos testar no exemplo acima:</p>
<pre class="r"><code>cross_val &lt;- cv.glmnet(x,y)

coef(cross_val)</code></pre>
<pre><code>## 51 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       1
## (Intercept) -0.01132651
## V1           0.92272709
## V2           0.97664814
## V3           0.91874905
## V4           0.93135112
## V5           0.95848594
## V6           0.90495909
## V7           0.95479111
## V8           0.97526108
## V9           0.94140976
## V10          0.94258243
## V11          .         
## V12          .         
## V13          .         
## V14          .         
## V15          .         
## V16          .         
## V17          .         
## V18          .         
## V19          .         
## V20          .         
## V21          .         
## V22          .         
## V23          .         
## V24          .         
## V25          .         
## V26          .         
## V27          .         
## V28          .         
## V29          .         
## V30          .         
## V31          .         
## V32          .         
## V33          .         
## V34          .         
## V35          .         
## V36          .         
## V37          .         
## V38          .         
## V39          .         
## V40          .         
## V41          .         
## V42          .         
## V43          .         
## V44          .         
## V45          .         
## V46          .         
## V47          .         
## V48          .         
## V49          .         
## V50          .</code></pre>
<p>O LASSO com CV põe apenas um coeficiente a mais. O <code>cv.glmnet</code> também fornece um plot com o número de coeficientes e o erro médio do Cross Validation, o que ajuda a ilustrar o ponto:</p>
<pre class="r"><code>plot(cross_val)</code></pre>
<p><img src="/post/LASSO/2018-09-01-LASSO_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Uma das linhas tracejadas marca o número de coeficientes escolhidos: 11, número em cima. Depois dessa linha tracejada, o <em>Mean Square Error</em> não flutua muito. E também não há muita variação para imediatamente a direita, onde temos o modelo verdadeiro com 10 coeficientes.</p>
<p><em>(O autor agradece aos Professores Marcelo Medeiros e Pedro Souza pelas longas explicações sobre o LASSO e métodos correlatos. Pedro Cava, o outro autor deste blog, foi primordial em incentivar esse texto. Todos os erros neste texto são, como de praxe, de minha exclusiva culpa)</em></p>
</div>

                        </div><div class="tags my-3"><a class="badge badge-pill badge-light border mr-2" href="/tags/lasso">
                                    <i class="fas fa-tag mr-2"></i>LASSO
                                </a><a class="badge badge-pill badge-light border mr-2" href="/tags/machine-learning">
                                    <i class="fas fa-tag mr-2"></i>Machine Learning
                                </a><a class="badge badge-pill badge-light border mr-2" href="/tags/r">
                                    <i class="fas fa-tag mr-2"></i>R
                                </a></div><ul class="share nav my-3 justify-content-end">
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://twitter.com/intent/tweet?url=%2f2018%2f09%2f16%2flasso%2f&text=O%20LASSO">
              <i class="fa-fw fab fa-twitter"></i>
          </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://www.linkedin.com/shareArticle?url=%2f2018%2f09%2f16%2flasso%2f&title=O%20LASSO">
                <i class="fa-fw fab fa-linkedin-in"></i>
            </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://www.facebook.com/sharer.php?u=%2f2018%2f09%2f16%2flasso%2f&t=O%20LASSO">
                <i class="fa-fw fab fa-facebook-f"></i>
            </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://reddit.com/submit?url=%2f2018%2f09%2f16%2flasso%2f&title=O%20LASSO">
                <i class="fa-fw fab fa-reddit-alien"></i>
            </a>
        </li>
    </nav>
                    </div>
                </div>

                <div class="row justify-content-center">
                    <div class="col-lg-8">
                        
                    </div>
                </div></article>
        </div>
    </div>

    <div class="related-content row mt-5 row-cols-1 row-cols-lg-3"><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/2018/09/11/solow/" class="d-block"><div class="card-body">
            <h4 class="card-title">Explorando o Modelo de Solow com a ajuda do R</h4>
            <p class="card-text text-muted text-uppercase">September 11, 2018</p>
            <div class="card-text">
                Em fevereiro de 1956 foi publicado no Quarterly Journal of Economics o trabalho A Contribution to the Theory of Economic Growth, de Robert Solow. Segundo o Google Scholar o paper acumulou cerca de 26000 citações de lá para cá e isso não deve ser uma grande surpresa.
Apesar de já existirem à época trabalhos importantes na área, como o de Ramsey (1928), Solow é quase um fundador da moderna teoria do crescimento econômico e por suas contribuições importantíssimas à essa literatura foi laureado com o Prêmio Nobel de Economia em 1987.
            </div>
        </div>
    </a>
</div>

            </div><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/2018/09/11/solow/" class="d-block"><div class="card-body">
            <h4 class="card-title">Explorando o Modelo de Solow com a ajuda do R</h4>
            <p class="card-text text-muted text-uppercase">September 11, 2018</p>
            <div class="card-text">
                Em fevereiro de 1956 foi publicado no Quarterly Journal of Economics o trabalho A Contribution to the Theory of Economic Growth, de Robert Solow. Segundo o Google Scholar o paper acumulou cerca de 26000 citações de lá para cá e isso não deve ser uma grande surpresa.
Apesar de já existirem à época trabalhos importantes na área, como o de Ramsey (1928), Solow é quase um fundador da moderna teoria do crescimento econômico e por suas contribuições importantíssimas à essa literatura foi laureado com o Prêmio Nobel de Economia em 1987.
            </div>
        </div>
    </a>
</div>

            </div><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/2018/09/01/microeconomia/" class="d-block"><div class="card-body">
            <h4 class="card-title">Um pouco de microeconomia, dualidade e R</h4>
            <p class="card-text text-muted text-uppercase">September 1, 2018</p>
            <div class="card-text">
                No meu segundo período da graduação em economia entrei em contato com a área que hoje me fascina, a cadeira era Teoria Micreconômica I. Ali tive um gostinho - à custa de algum sofrimento com listas e provas, confesso - do que é microeconomia. A cadeira tinha duas seções. A primeira era teoria da firma, a segunda, teoria do consumidor.
Estudamos os canônicos modelos neoclássicos de como uma firma escolhe sua planta e como um consumidor escolhe suas cestas de consumo.
            </div>
        </div>
    </a>
</div>

            </div></div>
</main>


    <footer class="footer text-center bg-dark py-6">
    <div class="container">
        <div class="row">
            <div class="col">
                <ul class="list-inline">
                    <li class="list-inline-item"><a href="/index.xml" rel="alternate" type="application/rss+xml" class="icons d-block">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a></li><li class="list-inline-item">
                            <a href="https://github.com/danmrc/azul/tree/master/C%C3%B3digos" class="icons d-block">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                </ul>

                <p class="text-muted">
                    
                        Copyright © 2008–2020, Pedro Cavalcante & Daniel Coutinho; all rights reserved.
                    
                </p>

                <p class="text-muted">
                Powered by <a href="https://gohugo.io" target="_blank">Hugo</a> with <a href="https://github.com/puresyntax71/hugo-theme-chunky-poster" target="_blank">Chunky Poster</a>.
                </p>
            </div>
        </div>
    </div>
</footer>

    
    
        
            <script src="/dist/main.d608eadfe5ac0688902e.min.js"></script>
        
    






<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>

<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-123754589-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

</body>
</html>
