<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<title>
  
     Jogo da Velha com Q-Learning | 
    AZUL
  
</title><meta name="description" content="Economia, Estatística, Programação"><meta name="author" content="pedrocava">

<link rel="apple-touch-icon" href="/apple-touch-icon.png" sizes="180x180">
<link rel="icon" href="/favicon-32x32.png " sizes="32x32" type="image/png">
<link rel="icon" href="/favicon-16x16.png" sizes="16x16" type="image/png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#0c344b">
<link rel="icon" href="/favicon.ico">


    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/themes/prism-okaidia.min.css">
    



    
        
            <link rel="stylesheet" href="/dist/main.37ab3f61b95417873748.min.css">
        
    




<link rel="canonical" href="/post/reforcoaprendizado/aprendizado/"><meta property="og:title" content="Jogo da Velha com Q-Learning" />
<meta property="og:description" content="Aqui no blog já abordamos várias vezes técnicas que podemos colocar na caixinha do Aprendizado Supervisionado - onde praticamente todo o ferramental da Econometria está. Também abordamos Aprendizado Não-Supervisionado quando falamos de clustering k-means. Acho que vale agora por o dedinho na água do Aprendizado por Reforço. Deixo o aviso de que apesar de falarmos que abordamos o conteúdo aqui da maneira como gostaríamos de ao assunto ter sido apresentados, definitivamente não é assim que eu gostaria de ter sido introduzido a Aprendizado por Reforço porque, bem, eu não fui introduzido a esse mundo, não de verdade." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/reforcoaprendizado/aprendizado/" />
<meta property="article:published_time" content="2020-04-21T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-04-21T00:00:00+00:00" />
<meta itemprop="name" content="Jogo da Velha com Q-Learning">
<meta itemprop="description" content="Aqui no blog já abordamos várias vezes técnicas que podemos colocar na caixinha do Aprendizado Supervisionado - onde praticamente todo o ferramental da Econometria está. Também abordamos Aprendizado Não-Supervisionado quando falamos de clustering k-means. Acho que vale agora por o dedinho na água do Aprendizado por Reforço. Deixo o aviso de que apesar de falarmos que abordamos o conteúdo aqui da maneira como gostaríamos de ao assunto ter sido apresentados, definitivamente não é assim que eu gostaria de ter sido introduzido a Aprendizado por Reforço porque, bem, eu não fui introduzido a esse mundo, não de verdade.">
<meta itemprop="datePublished" content="2020-04-21T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-04-21T00:00:00+00:00" />
<meta itemprop="wordCount" content="953">



<meta itemprop="keywords" content="R,Matemática,Reinforcement Learning,Simulações," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Jogo da Velha com Q-Learning"/>
<meta name="twitter:description" content="Aqui no blog já abordamos várias vezes técnicas que podemos colocar na caixinha do Aprendizado Supervisionado - onde praticamente todo o ferramental da Econometria está. Também abordamos Aprendizado Não-Supervisionado quando falamos de clustering k-means. Acho que vale agora por o dedinho na água do Aprendizado por Reforço. Deixo o aviso de que apesar de falarmos que abordamos o conteúdo aqui da maneira como gostaríamos de ao assunto ter sido apresentados, definitivamente não é assim que eu gostaria de ter sido introduzido a Aprendizado por Reforço porque, bem, eu não fui introduzido a esse mundo, não de verdade."/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>



</head>
<body>
    
<nav class="navbar navbar-expand-md navbar-light bg-light fixed-top shadow-sm" id="navbar-main-menu">
    <div class="container">
        <a class="navbar-brand font-weight-bold" href="/">AZUL</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-menu" aria-controls="main-menu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="main-menu">
            <ul class="navbar-nav ml-auto">
                
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/categories/">Categorias</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/about/">Sobre</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/tags/">Tags</a></li>
                
            
            </ul>
        </div>
    </div>
</nav>


    
<main class="content-page container pt-7 pb-5">
    
    <div class="row">
        <div class="col">
            <article>
                <div class="row justify-content-center">
                    <div class="col-lg-8">
                        <div class="meta text-muted mb-3">
                            <p class="created text-muted text-uppercase font-weight-bold mb-1">April 21, 2020</p>
                            <span class="mr-2"><i class="fas fa-book-open mr-2"></i>953 words</span>
                            <span><i class="fas fa-clock mr-2"></i>5 mins read</span>
                        </div>

                        <h1>Jogo da Velha com Q-Learning</h1>

                        <ul class="authors list-inline"><li class="list-inline-item mr-3">
                    <div class="media author"><div class="media-body">
                            <h5 class="name my-0"><a href="/authors/pedrocava/" class="small">Pedro Cavalcante</a>
                            </h5><p class="social small text-muted">
                                    <a href="https://twitter.com/@pedroocava">@PedrooCava</a>
                                </p></div>
                    </div>
                </li></ul>
                    </div>
                </div><div class="row justify-content-center">
                    <div class="col-lg-8">
                        <div class="content">
                            
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>Aqui no blog já abordamos várias vezes técnicas que podemos colocar na caixinha do Aprendizado Supervisionado - onde praticamente todo o ferramental da Econometria está. Também abordamos Aprendizado Não-Supervisionado quando falamos de clustering k-means. Acho que vale agora por o dedinho na água do Aprendizado por Reforço. Deixo o aviso de que apesar de falarmos que abordamos o conteúdo aqui da maneira como gostaríamos de ao assunto ter sido apresentados, definitivamente não é assim que eu gostaria de ter sido introduzido a Aprendizado por Reforço porque, bem, eu <em>não</em> fui introduzido a esse mundo, não de verdade. Tenho estudado por conta própria para atacar um problema interessantíssimo no trabalho e pensei em deixar uma introdução prática, um cheiro do assunto, aqui.</p>
<p>Qual é a nossa motivação? Bem, uma série de situações podem ser descritas como: suponha que você observe um sistema com estados variáveis e um agente, a quem podemos associar em cada momento do tempo duas grandezas, a ação <em>no próximo período</em> e a recompensa <em>no período atual</em>. A recompensa é uma função do estado da natureza e da ação. Queremos agora aprender qual regra de comportamento induz alguma forma de maximização de recompensa. Os paralelos com microeconomia já estão surgindo na sua mente, leitor? A ideia de Aprendizado por Reforço (e mais especificamente, <span class="math inline">\(Q\)</span>-learning, a técnica que usarei) é expor um agente a uma série potencialmente repetida de trios estado-ação-recompensa e daí aprender a resposta ótima.</p>
<p>Temos um conjunto de estados <span class="math inline">\(S\)</span>, um conjunto de potenciais ações <span class="math inline">\(A\)</span>, um conjunto de potenciais recompensas <span class="math inline">\(R\)</span>. Cada iteração <span class="math inline">\(i\)</span> do sistema tem um estado <span class="math inline">\(s_i \in S\)</span>, uma ação <span class="math inline">\(a_i\)</span> dentre as ações possíveis para o estado, <span class="math inline">\(A(s_i)\)</span>, e <span class="math inline">\(a_i\)</span> determina uma recompensa <span class="math inline">\(r_{i+1}\)</span> e um estado <span class="math inline">\(s_{i+1}\)</span>. Vamos então definir a função <span class="math inline">\(Q \, : S \times A \to \mathbb{R}\)</span> que associa a cada par <span class="math inline">\((s_i, a_i)\)</span> uma recompensa esperada.</p>
<p>Felizmente a página da Wikipedia sobre <span class="math inline">\(Q\)</span>-learning tem uma excelente “equação comentada” descrevendo o algoritimo e vou reproduzi-la com algumas alterações aqui embaixo.</p>
<p><span class="math display">\[ \underbrace{Q(s_{t},a_{t})}_{\text{valor antigo}} + \underbrace{\alpha}_{\text{taxa de aprendizado}} \cdot  \bigg( \underbrace{r_{t}}_{\text{recompensa}} + \underbrace{\gamma}_{\text{fator de desconto}} \cdot \underbrace{\max_{a}Q(s_{t+1}, a)}_{\text{melhor ação no próximo período}} - \,\underbrace{Q(s_{t},a_{t})}_{\text{valor antigo}} \bigg)  \rightarrow Q(s_{t},a_{t})  \]</span></p>
<p>Apareceram dois parâmetros novos:</p>
<ul>
<li>Taxa de Aprendizado, <span class="math inline">\(\alpha\)</span></li>
</ul>
<p>Um número real entre <span class="math inline">\(0\)</span> e <span class="math inline">\(1\)</span> que determina que fração do “aprendizado” com alternativas não tomadas é incorporado.</p>
<ul>
<li>Fator de Desconto, <span class="math inline">\(\gamma\)</span></li>
</ul>
<p>Outro escalar entre <span class="math inline">\(0\)</span> e <span class="math inline">\(1\)</span>, determina o peso associado ao futuro. Treinar com um <span class="math inline">\(\gamma\)</span> mais próximo de <span class="math inline">\(1\)</span> implica ponderar mais nas regras de resposta o longo prazo.</p>
<p>Em breve aparecerá outro parâmetro, <span class="math inline">\(\epsilon\)</span>. A ideia é que podemos, de vez em quando, aleatoriezar a ação a ser tomada pelo algoritmo para tentar conhecer melhor as consequências de cursos alternativos. A cada iteração, com probabilidade <span class="math inline">\(1-\epsilon\)</span> podemos aleatorizar a ação, por exemplo.</p>
<p>Os dados são adaptados de <span class="citation">Sutton and Barto (2018)</span>. Todos são coletados da perspectiva do jogador X - que está contra B e jogou primeiro. X ganha <span class="math inline">\(+1\)</span> se ganha, <span class="math inline">\(0\)</span> se empata e <span class="math inline">\(-1\)</span> se perde. O quadro é preenchido lendo da esquerda para a direita. As primeiras três entradas da <em>string</em> <code>State</code> representam as três primeiras entradas, as três seguintes representam a segunda linha e as últimas três entradas, a última linha do tabuleiro. As ações são sempre “c” seguido de um número. O número sinaliza em qual altura da string devemos inserir a letra. “c3” implica por a letra no canto direito superior do tabuleiro, “c7” no canto inferior esquerdo, “c5” no meio, “c9” no canto inferior direito, etc…</p>
<pre class="r"><code>library(dplyr)
library(ReinforcementLearning)

data(&quot;tictactoe&quot;)

(dados &lt;- as_tibble(tictactoe))</code></pre>
<pre><code>## # A tibble: 406,541 x 4
##    State     Action NextState Reward
##    &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;
##  1 ......... c7     ......X.B      0
##  2 ......X.B c6     ...B.XX.B      0
##  3 ...B.XX.B c2     .XBB.XX.B      0
##  4 .XBB.XX.B c8     .XBBBXXXB      0
##  5 .XBBBXXXB c1     XXBBBXXXB      0
##  6 ......... c1     X...B....      0
##  7 X...B.... c4     X..XB.B..      0
##  8 X..XB.B.. c3     XBXXB.B..      0
##  9 XBXXB.B.. c8     XBXXBBBX.      0
## 10 XBXXBBBX. c9     XBXXBBBXX      0
## # … with 406,531 more rows</code></pre>
<pre class="r"><code>model &lt;- ReinforcementLearning(dados, 
                               s = &quot;State&quot;, 
                               a = &quot;Action&quot;, 
                               r = &quot;Reward&quot;, 
                               s_new = &quot;NextState&quot;, 
                               iter = 1, # quantas vezes repetimos os dados 
                               control = list(alpha = 0.2, 
                                              gamma = 0.4, 
                                              epsilon = 0.1)) # aqui damos os parâmetros</code></pre>
<p>Agora podemos simular situações nunca antes vistas e a melhor resposta. O algoritmo, inclusive, aprendeu a tática comum de começar pelo meio:</p>
<pre class="r"><code>predict(model, &quot;.........&quot;)</code></pre>
<pre><code>## [1] &quot;c5&quot;</code></pre>
<p>É uma boa ideia se ater ao paradigma funcional de R, então vamos fazer uma função que receba um modelo e um estado em forma de string. Ela irá computar a resposta, dar uma resposta (aleatória) do <em>outro</em> jogador e devolver o tabuleiro atualizado em uma string.</p>
<pre class="r"><code>move &lt;- function(model, state) {
  
  policy &lt;- predict(model, state) %&gt;%
    stringr::str_sub(2) %&gt;%
    as.numeric()
  
  stringr::str_sub(state, policy, policy) &lt;- &quot;X&quot;
  
  bPolicy &lt;- stringr::str_split(state, &quot;&quot;)[[1]] %&gt;%
    stringr::str_which(&#39;\\.&#39;) %&gt;%
    sample(1)
  
  stringr::str_sub(state, bPolicy, bPolicy) &lt;- &quot;B&quot;

  state
    
}</code></pre>
<p>Agora podemos simular um jogo. Observe que a função foi feita para compor recursivamente, mas isso não é lá muito elegante… Note também que podemos gerar alguns estados inválidos caso a escolha do jogador B tenha sido particularmente estranha.</p>
<pre class="r"><code>set.seed(12345)
move(model, &quot;.........&quot;)</code></pre>
<pre><code>## [1] &quot;....X.B..&quot;</code></pre>
<pre class="r"><code>move(model, move(model, &quot;.........&quot;))</code></pre>
<pre><code>## [1] &quot;X.BBX....&quot;</code></pre>
<pre class="r"><code>move(model, move(model, move(model, &quot;.........&quot;)))</code></pre>
<pre><code>## [1] &quot;BXBBX..X.&quot;</code></pre>
<p>Um exercício legal a partir daqui é usar <code>purrr::accumulate</code>, como eu mostro no post <a href="https://azul.netlify.app/2020/02/09/difusao-gaussiana/">Gerando um Padrão de Difusão</a>, para compor recursivamente <code>move</code> de maneira programática e declarativa e obter a trajetória do jogo, ou <code>purrr::reduce</code> para fazer a mesma operação, porém reduzindo uma condição inicial à uma final, perdendo a trajetória do jogo.</p>
<div id="bibliografia" class="section level1 unnumbered">
<h1>Bibliografia</h1>
<div id="refs" class="references hanging-indent">
<div id="ref-sutton2018">
<p>Sutton, Richard S, and Andrew G Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. MIT press.</p>
</div>
</div>
</div>

                        </div><div class="tags my-3"><a class="badge badge-pill badge-light border mr-2" href="/tags/r">
                                    <i class="fas fa-tag mr-2"></i>R
                                </a><a class="badge badge-pill badge-light border mr-2" href="/tags/matem%C3%A1tica">
                                    <i class="fas fa-tag mr-2"></i>Matemática
                                </a><a class="badge badge-pill badge-light border mr-2" href="/tags/reinforcement-learning">
                                    <i class="fas fa-tag mr-2"></i>Reinforcement Learning
                                </a><a class="badge badge-pill badge-light border mr-2" href="/tags/simula%C3%A7%C3%B5es">
                                    <i class="fas fa-tag mr-2"></i>Simulações
                                </a></div><ul class="share nav my-3 justify-content-end">
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://twitter.com/intent/tweet?url=%2fpost%2freforcoaprendizado%2faprendizado%2f&text=Jogo%20da%20Velha%20com%20Q-Learning">
              <i class="fa-fw fab fa-twitter"></i>
          </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://www.linkedin.com/shareArticle?url=%2fpost%2freforcoaprendizado%2faprendizado%2f&title=Jogo%20da%20Velha%20com%20Q-Learning">
                <i class="fa-fw fab fa-linkedin-in"></i>
            </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://www.facebook.com/sharer.php?u=%2fpost%2freforcoaprendizado%2faprendizado%2f&t=Jogo%20da%20Velha%20com%20Q-Learning">
                <i class="fa-fw fab fa-facebook-f"></i>
            </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://reddit.com/submit?url=%2fpost%2freforcoaprendizado%2faprendizado%2f&title=Jogo%20da%20Velha%20com%20Q-Learning">
                <i class="fa-fw fab fa-reddit-alien"></i>
            </a>
        </li>
    </nav>
                    </div>
                </div>

                <div class="row justify-content-center">
                    <div class="col-lg-8">
                        
                    </div>
                </div></article>
        </div>
    </div>

    <div class="related-content row mt-5 row-cols-1 row-cols-lg-3"><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/post/industria/" class="d-block"><div class="card-body">
            <h4 class="card-title">Mas e a indústria?</h4>
            <p class="card-text text-muted text-uppercase">April 12, 2020</p>
            <div class="card-text">
                Dia desses li coisas tristes. A narrativa era de que alguns setores são por alguma propriedade vinda dos céus (alguns dirão ah mas e a complexidade… e eu direi que são eles os que invejam os físicos) mais “importantes” que outros e que, de fato, o processo de desenvolvimento econômico é sim substituir participação de setores menos complexos por outros mais complexos. A magia, o pulo do gato, o estopim de um ciclo virtuoso de crescimento estaria em produzir menos soja e mais massa proteica, menos ferro e mais carros, menos bananas e mais microchips… Qualquer semelhança com as viúvas do regime militar não é coincidência.
            </div>
        </div>
    </a>
</div>

            </div><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/post/randomwalkemdoisd/difusao-gaussiana/" class="d-block"><div class="card-body">
            <h4 class="card-title">Gerando um padrão de difusão com soma de um termo gaussiano</h4>
            <p class="card-text text-muted text-uppercase">February 9, 2020</p>
            <div class="card-text">
                Dia desses eu fui informado por um amigo de que um padrão bonitinho de difusão acontece somando um termo gaussiano acumuladamente a um conjunto. O que o amigo versado em Física me relatou como um “padrão de difusão”, na minha intuição mais econométrica vem como uma random walk no \(\mathbb{R}^2\).
Bem, vamos usar o purrr e o dplyr para gerar de maneira concisa um tibble pronto para ser passado ao ggplot2.
            </div>
        </div>
    </a>
</div>

            </div><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/post/teoremamacacoinfinito/macaco-infinito-hamlet/" class="d-block"><div class="card-body">
            <h4 class="card-title">O Teorema do Macaco Infito: quanto tempo até sair Hamlet?</h4>
            <p class="card-text text-muted text-uppercase">September 8, 2019</p>
            <div class="card-text">
                O Enunciado e Quase-Certeza  Probabilidades de palavras em particular com alfabetos finitos  Simulação library(dplyr) library(tibble) library(rio) palavras &amp;lt;- import(&amp;quot;https://github.com/pythonprobr/palavras/blob/master/palavras.txt?raw=true&amp;quot;) %&amp;gt;% as_tibble() palavras$tamanho &amp;lt;- stringr::str_length(palavras$a) # tamanho das palavras Existem maneiras mais elegantes de armazenar os resultados desta simulação, mas eu fiz isso com pressa e - convenhamos - isso aqui é só um blog. Vamos ao passo a passo do desenho da simulação. Primeiro definimos parâmetros e objetos:
            </div>
        </div>
    </a>
</div>

            </div></div>
</main>


    <footer class="footer text-center bg-dark py-6">
    <div class="container">
        <div class="row">
            <div class="col">
                <ul class="list-inline">
                    <li class="list-inline-item"><a href="/index.xml" rel="alternate" type="application/rss+xml" class="icons d-block">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a></li><li class="list-inline-item">
                            <a href="https://github.com/danmrc/azul/tree/master/C%C3%B3digos" class="icons d-block">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                </ul>

                <p class="text-muted">
                    
                        Copyright © 2008–2020, Pedro Cavalcante & Daniel Coutinho; all rights reserved.
                    
                </p>

                <p class="text-muted">
                Powered by <a href="https://gohugo.io" target="_blank">Hugo</a> with <a href="https://github.com/puresyntax71/hugo-theme-chunky-poster" target="_blank">Chunky Poster</a>.
                </p>
            </div>
        </div>
    </div>
</footer>

    
    
        
            <script src="/dist/main.d608eadfe5ac0688902e.min.js"></script>
        
    



<script>
    window.Prism = window.Prism || {};
    window.Prism.manual = true;
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-core.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/plugins/autoloader/prism-autoloader.min.js"></script>






    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-123754589-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

</body>
</html>
